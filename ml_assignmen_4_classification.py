# -*- coding: utf-8 -*-
"""ml-assignmen-4-classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lHySzOzd3HphhCiIhS_x7AXqyIKkZ1Z0

# **üì• Import Libraries**
"""

import pandas as pd
import numpy as np

from sklearn.datasets import load_breast_cancer

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt

import seaborn as sns

"""# üß™ Load Breast Cancer Dataset"""

# Load Breast Cancer Dataset
data = load_breast_cancer()

df = pd.DataFrame(data.data, columns=data.feature_names)

df['target'] = data.target


#  Check for Missing Values
print("Missing values:\n", df.isnull().sum())


# EDA: Distribution of Target Classes
sns.countplot(data=df, x='target')

plt.title('Class Distribution (0 = Malignant, 1 = Benign)')

plt.show()

# Split into Features & Target
X = df.drop('target', axis=1)

y = df['target']

# Standardize Features
scaler = StandardScaler()

X_scaled = scaler.fit_transform(X)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""## üß† Model Training and Hyperparameter Tuning

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=10000)

lr.fit(X_train, y_train)

from sklearn.model_selection import GridSearchCV, cross_val_score

# Logistic Regression GridSearch
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['lbfgs']
}

grid_lr = GridSearchCV(LogisticRegression(max_iter=10000), param_grid_lr, cv=5, scoring='accuracy')

grid_lr.fit(X_train, y_train)

lr = grid_lr.best_estimator_

print("Best Logistic Regression Parameters:", grid_lr.best_params_)

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier


dt = DecisionTreeClassifier(random_state=42)

dt.fit(X_train, y_train)

from sklearn.tree import DecisionTreeClassifier

from sklearn.model_selection import GridSearchCV, cross_val_score

# Hyperparameter tuning for Decision Tree Classifier
dt_params = {
    'max_depth': [3, 5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_params, cv=5, scoring='accuracy')

dt_grid.fit(X_train, y_train)

print("Best Params (Decision Tree):", dt_grid.best_params_)

print("Best CV Score (Decision Tree):", dt_grid.best_score_)


dt = dt_grid.best_estimator_

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import GridSearchCV, cross_val_score

# RF Hyperparameters
rf_params = {
    'n_estimators': [100, 150],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5]
}

rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='r2', verbose=1, n_jobs=-1)

rf_grid.fit(X_train, y_train)

print("Best Params (RF):", rf_grid.best_params_)

print("Best CV Score (RF):", rf_grid.best_score_)


rf = rf_grid.best_estimator_


rf_cv = cross_val_score(rf, X_train, y_train, cv=5, scoring='r2')

print("RF CV R¬≤ mean:", rf_cv.mean())

"""## SVM"""

from sklearn.svm import SVC

svm = SVC(kernel='rbf', probability=True)

svm.fit(X_train, y_train)

from sklearn.svm import SVC

from sklearn.model_selection import GridSearchCV, cross_val_score

# SVC GridSearchCV
svc_params = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf'],
    'gamma': ['scale', 'auto']
}

svc_grid = GridSearchCV(SVC(probability=True), svc_params, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)

svc_grid.fit(X_train, y_train)

print("Best Params (SVC):", svc_grid.best_params_)

print("Best CV Score (SVC):", svc_grid.best_score_)


svm = svc_grid.best_estimator_


svc_cv = cross_val_score(svm, X_train, y_train, cv=5, scoring='accuracy')

print("SVC CV Accuracy mean:", svc_cv.mean())

"""## kNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

#  k-NN GridSearch
param_grid_knn = {
    'n_neighbors': list(range(3, 11)),
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski', 'euclidean', 'manhattan']
}

grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='accuracy')

grid_knn.fit(X_train, y_train)

knn = grid_knn.best_estimator_

print("Best k-NN Parameters:", grid_knn.best_params_)

"""## üìà Model Evaluation"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

from sklearn.model_selection import GridSearchCV

import seaborn as sns

import matplotlib.pyplot as plt

results = []
models = {
    "Logistic Regression": lr,
    "Decision Tree": dt,
    "Random Forest": rf,
    "SVM": svm,
    "k-NN": knn
}

for name, model in models.items():
    y_pred = model.predict(X_test)


    if y_pred.dtype != int and y_pred.dtype != bool:
        y_pred = np.round(y_pred)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1-Score': f1
    })


results_df = pd.DataFrame(results).sort_values(by='F1-Score', ascending=False)
print(results_df)

plt.figure(figsize=(10, 6))
sns.barplot(data=results_df, x='F1-Score', y='Model', palette='viridis')
plt.title('F1 Score Comparison Across Classifiers', fontsize=14)
plt.xlabel('F1 Score')
plt.ylabel('Model')
plt.grid(axis='x')
plt.tight_layout()
plt.show()

from sklearn.metrics import ConfusionMatrixDisplay

best_model_name = results_df.iloc[0]['Model']
best_model = models[best_model_name]

ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, cmap='Blues')
plt.title(f'Confusion Matrix - {best_model_name}')
plt.show()

"""## üèÅ Conclusion:

****‚úÖ The best-performing model was Logistic Regression with an F1 score of 0.9790, closely tied with SVM, which had the same F1 score.**

**‚úÖ Logistic Regression and k-NN both showed noticeable performance gains after GridSearchCV hyperparameter tuning.**

**‚úÖ All models performed very well with F1-scores above 0.95, but Logistic Regression and SVM led the pack with superior precision-recall balance.**

‚úÖ The confusion matrix for Logistic Regression shows excellent classification:  


**Only 3 misclassifications out of 114 samples**

**Strong class separation with minimal false negatives**


**‚úÖ Simpler models like k-NN are still effective but can be slower to predict on large datasets due to distance calculations.**

**‚úÖ Random Forest and Decision Tree also performed well, but were slightly behind in F1-score due to minor precision-recall tradeoffs.**

"""

